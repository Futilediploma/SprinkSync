# ü§ñ SprinkSync LLM Integration Setup Guide

## Overview

SprinkSync now features **Local LLM Intelligence** using Ollama to dramatically improve fire protection activity detection. The AI understands vague construction terminology and contextual clues that traditional keyword matching misses.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PDF Schedule      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Keyword-Based Parser       ‚îÇ
‚îÇ  (Initial Pass)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Enhancement Layer      ‚îÇ
‚îÇ  - Context Analysis         ‚îÇ
‚îÇ  - Reasoning Engine         ‚îÇ
‚îÇ  - Confidence Scoring       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Enhanced Results + UI      ‚îÇ
‚îÇ  - AI Reasoning Display     ‚îÇ
‚îÇ  - User Feedback Loop       ‚îÇ
‚îÇ  - Confidence Indicators    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Installation

### 1. Install Ollama

**Windows:**
```powershell
# Download installer from https://ollama.com/download
# Or use winget
winget install Ollama.Ollama
```

**macOS:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Pull the LLM Model

```bash
# Recommended: Llama 3.2 (3B) - Fast and accurate
ollama pull llama3.2

# Alternative: Mistral 7B - Higher accuracy, slower
ollama pull mistral

# Alternative: Phi-3 - Very fast, slightly less accurate
ollama pull phi3
```

### 3. Start Ollama Service

```bash
ollama serve
```

The service runs on `http://localhost:11434` by default.

### 4. Install Dependencies

```bash
# From schedule-extractor directory
npm install

# From schedule-extractor/server directory
cd server
npm install
```

### 5. Start the Backend

```bash
# From schedule-extractor/server directory
npm start
```

Backend runs on `http://localhost:3001`

### 6. Start the Frontend

```bash
# From schedule-extractor directory
npm run dev
```

Frontend runs on `http://localhost:5173`

## How It Works

### 1. **Keyword-Based Initial Pass**
The parser first uses traditional keyword matching to identify potential fire protection activities. This provides a baseline set of activities.

### 2. **LLM Context Analysis**
For each activity, the LLM receives:
- The activity name
- Surrounding activities (before/after)
- Project type
- Comprehensive prompt with fire protection context

### 3. **Intelligent Classification**
The LLM reasons about:
- **MEP Context**: "MEP Overhead Rough" ‚Üí likely includes sprinklers
- **Spatial Indicators**: "Level 2" ‚Üí riser and branch line work
- **Trade Sequencing**: Activity between slab and inspection ‚Üí probable sprinkler work
- **Testing Patterns**: "Hydro test" after rough-in ‚Üí definitely sprinkler testing
- **Equipment Keywords**: "Backflow", "FDC", "Fire Pump" ‚Üí clear indicators

### 4. **Confidence Scoring**
The LLM provides a confidence score (0.0 to 1.0) based on:
- Clarity of indicators
- Context strength
- Pattern recognition

### 5. **User Feedback Loop**
Users can mark classifications as correct/incorrect, which:
- Invalidates cached responses
- Logs corrections for future fine-tuning
- Improves accuracy over time

## API Endpoints

### Health Check
```http
GET /api/health
```

Response:
```json
{
  "status": "ok",
  "ollama_connected": true,
  "model_available": true,
  "cache_keys": 42
}
```

### Classify Single Activity
```http
POST /api/classify-activity
Content-Type: application/json

{
  "activity": "MEP Overhead Rough-in Level 2",
  "context": ["Electrical rough complete", "Inspection scheduled"],
  "projectType": "commercial"
}
```

Response:
```json
{
  "is_fire_protection": true,
  "confidence": 0.85,
  "category": "rough_in",
  "reasoning": "MEP overhead rough-in typically includes sprinkler mains and branch lines...",
  "suggestion": "Sprinkler overhead rough-in for Level 2",
  "phase": "Overhead Rough-in"
}
```

### Batch Classification
```http
POST /api/classify-batch
Content-Type: application/json

{
  "activities": [
    {
      "activity": "Level 3 MEP Coordination",
      "context": ["Structural complete", "Drywall starting"]
    },
    "Underground Utilities",
    "Ceiling Grid Installation"
  ],
  "projectType": "commercial"
}
```

### Enhance Existing Activities
```http
POST /api/enhance-activities
Content-Type: application/json

{
  "activities": [
    {
      "name": "Overhead rough Level 2",
      "startDate": "01/15/2024",
      "finishDate": "01/22/2024"
    }
  ],
  "projectType": "commercial"
}
```

Response includes original activities enhanced with LLM fields:
```json
{
  "activities": [...],
  "total": 15,
  "fire_protection_count": 12
}
```

### Submit User Correction
```http
POST /api/learn-correction
Content-Type: application/json

{
  "activity": "MEP Coordination Meeting",
  "context": ["Site walkthrough", "RFI review"],
  "wasFireProtection": true,
  "shouldBeFireProtection": false,
  "userNote": "This was just a meeting, no actual work"
}
```

### Cache Management
```http
GET /api/cache-stats
POST /api/cache-clear
```

## Configuration

### Model Selection

Edit `server/index.js` to change the model:

```javascript
const response = await ollama.chat({
  model: 'llama3.2',  // Change to 'mistral' or 'phi3'
  messages: [{ role: 'user', content: prompt }],
  stream: false,
  options: {
    temperature: 0.3,  // Lower = more consistent (0.0-1.0)
    top_p: 0.9,        // Nucleus sampling threshold
  }
});
```

### Cache Settings

Adjust cache TTL in `server/index.js`:

```javascript
const cache = new NodeCache({ 
  stdTTL: 3600  // Cache duration in seconds (1 hour default)
});
```

### API Base URL

Update frontend API endpoint in `src/utils/llmService.ts`:

```typescript
const API_BASE_URL = 'http://localhost:3001/api'
```

## Performance Optimization

### 1. **Response Caching**
- Identical queries return cached results instantly
- Cache persists for 1 hour
- Reduces LLM load by ~70% on repeated schedules

### 2. **Batch Processing**
- Process multiple activities in sequence
- Progress logging every 5-10 activities
- Prevents overwhelming the LLM

### 3. **Context Window**
- Only 2 surrounding activities included (1 before, 1 after)
- Reduces token usage while maintaining context

### 4. **Temperature Control**
- Low temperature (0.3) for consistent results
- Reduces variation between identical queries

## Model Comparison

| Model | Size | Speed | Accuracy | Best For |
|-------|------|-------|----------|----------|
| **Llama 3.2** | 3B | ‚ö°‚ö°‚ö° Fast | üéØüéØüéØ Good | **Recommended** - Best balance |
| Mistral 7B | 7B | ‚ö°‚ö° Medium | üéØüéØüéØüéØ Excellent | Maximum accuracy |
| Phi-3 | 3.8B | ‚ö°‚ö°‚ö°‚ö° Very Fast | üéØüéØ Fair | Speed priority |

## Troubleshooting

### LLM Backend Not Available

**Issue:** Frontend shows "LLM backend not available"

**Solutions:**
1. Check Ollama is running: `ollama list`
2. Start Ollama service: `ollama serve`
3. Verify model installed: `ollama pull llama3.2`
4. Check backend server: `http://localhost:3001/api/health`

### Slow Classification

**Issue:** Activities taking too long to classify

**Solutions:**
1. Use a smaller model (llama3.2 instead of mistral)
2. Reduce batch size
3. Check system resources (RAM, CPU)
4. Clear cache if it's grown too large

### Incorrect Classifications

**Issue:** LLM misclassifying activities

**Solutions:**
1. Submit corrections via UI to improve learning
2. Check prompt template in `server/index.js`
3. Adjust temperature (lower = more consistent)
4. Provide better context (surrounding activities)
5. Consider switching to Mistral 7B for higher accuracy

### JSON Parsing Errors

**Issue:** "Invalid response structure from LLM"

**Solutions:**
1. Check LLM model is fully downloaded
2. Restart Ollama service
3. Clear cache: `POST /api/cache-clear`
4. Check console logs for malformed JSON

## Example Prompts

The LLM uses sophisticated prompts that include:

‚úÖ **MEP Context Understanding**
> "MEP rough-in often includes sprinkler piping"

‚úÖ **Spatial Reasoning**
> "Floor levels (L1, L2) indicate riser and branch line work"

‚úÖ **Trade Sequencing Logic**
> "Activities between slab complete and MEP inspection often include sprinklers"

‚úÖ **Testing Pattern Recognition**
> "Hydro test following rough-in suggests sprinkler systems"

‚úÖ **Equipment Identification**
> "Fire pumps, backflow preventers, FDC are clear indicators"

## Privacy & Security

### 100% Local Processing
- All data stays on your machine
- No cloud API calls
- No data sent to external servers
- Ollama runs completely offline

### Data Flow
```
Your Computer Only:
PDF ‚Üí Parser ‚Üí LLM (Local) ‚Üí Results
```

### GDPR/HIPAA Compliant
Since all processing is local, there are no data privacy concerns for sensitive project information.

## Future Enhancements

### Planned Features
- [ ] Fine-tuning on user corrections
- [ ] Custom model training for specific contractors
- [ ] Multi-model ensemble voting
- [ ] Automatic keyword expansion
- [ ] Project-specific learning profiles
- [ ] Export corrections dataset
- [ ] A/B testing different models

## Support

**Issues:** Open a GitHub issue with:
- LLM model used
- Example activity text
- Error messages
- `GET /api/health` response

**Performance Logs:**
Check browser console for detailed parsing logs:
```
üìÑ Starting PDF parsing
ü§ñ Enhancing activities with LLM intelligence...
‚ú® LLM enhancement complete: 12/15 confirmed FP activities
```

## Credits

Built with:
- [Ollama](https://ollama.com/) - Local LLM runtime
- [Llama 3.2](https://ai.meta.com/llama/) - Meta AI model
- [Express.js](https://expressjs.com/) - Backend framework
- [Node-Cache](https://github.com/node-cache/node-cache) - Response caching

---

**Made with ‚ù§Ô∏è for Fire Protection Professionals**
